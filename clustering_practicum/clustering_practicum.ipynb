{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Practicum \n",
    "\n",
    "\n",
    "## KMeans vs. Hierarchical Clustering (vs. KMedoids/PAM)\n",
    "\n",
    "### Setup\n",
    "\n",
    "Now that you have a lot of experience computing bags from words, let's do it one more time in Python where scikit-learn makes this a piece of cake using the functions: [CountVectorizer](http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage) and [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) ([reference](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer))\n",
    "\n",
    "We'll use KMeans, KMedoids, and hierarchical clustering to find patterns in text documents.  \n",
    "\n",
    "### K-means\n",
    "\n",
    "For this part of the exercise we will be using the [Fisher's Iris](http://archive.ics.uci.edu/ml/datasets/Iris) dataset.  Similar to the decision tree exercise, we will test our algorithm on a somewhat small (trivial) dataset to make sure the mechanics of our algorithm are functioning correctly before we apply it to a more substantial example. \n",
    "\n",
    "![kmean](images/kmeans.gif)\n",
    " (refresh/click to see animation)\n",
    "\n",
    "### Implementation and testing\n",
    "\n",
    "1. Implement KMeans (and KMedoids for reach goals) using the stubs (`*_stub.py`) you have been given. Please complete the stub and save the completed stub with a proper name, not `*_stub.py`. Do not turn in notebooks. Build your own proper testing harness with unit tests where needed. If you wish, you may use your own custom design for the software, as long as it follows the scikit API design (.fit() and .predict() methods).\n",
    "\n",
    "Often it is tough to pick an ideal k in advance.  We can force k in our case if we want a predetermined number of sections/topics.  But it is most likely better to vary k and let the algorithm tell us what it wants.  \n",
    "\n",
    "2. Run the algorithm with increasing values of k.  For each, compute the sum of squared error ([SSE](http://en.wikipedia.org/wiki/Residual_sum_of_squares)) and within cluster variance (WCV) using the formula given in the notes.  Plot both of these for each value of k and try to find an elbow. [Determining the number of clusters](http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set).  Is there an optimal # of K?\n",
    "\n",
    "3. Another metric to assess how well your data has been clustered is the Silhouette coefficient.  Using `scikit-learn's` metric package compute the [silhouette coefficient](http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient) of the clusters produced by your own Kmeans implementation on the iris data.\n",
    "\n",
    "4. Visualize the centroid assignments.  Create a plot of the cluster assignments on the iris data. \n",
    "\n",
    " ![](images/iris_cluster.png)\n",
    "\n",
    "5. Compare your cluster results with [scikit-learn Kmeans](http://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html#k-means-clustering).  Since K-means is a stochastic algorithm (random initialization) your result will be slightly (but hopefully not too) different.\n",
    "\n",
    "\n",
    "### Comparison to Hierarchical Clustering\n",
    "\n",
    "1. The first step to using scipy's [Hierarchical clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) (AgglomerativeClustering) is to first find out how similar our vectors are to one another. To do this we use the [pdist](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.pdist.html) function to compute a similarity matrix of our iris data (pairwise distances). First we will just use Euclidean distance. Examine the shape of what is returned.\n",
    "\n",
    "2. A quirk of pdist is that it returns a long vector. Use scipy's [squareform](http://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.spatial.distance.squareform.html) function to get our long vector of distances back into a square matrix. Look at the shape of this new matrix. Now that we have a square similarity matrix we can start to cluster. Pass this matrix into scipy's linkage function to compute hierarchical clusters.\n",
    "\n",
    "3. At this point, all the information about our clusters is complete but it is impossible to interpret in a sensible manner (the output format mimics that of MATLAB clustering). Thankfully scipy also has a function to visualize this madness. Using scipy's [dendrogram](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html) function plot the linkages as a hierachical tree.\n",
    "\n",
    "4. [Here](http://nbviewer.jupyter.org/github/herrfz/dataanalysis/blob/master/week3/hierarchical_clustering.ipynb) is a simple example of a heatmapped dendrogram with scipy, one of the most common, if not **the** most common, visualizations of professional clustering used in academia and certain industries.\n",
    "\n",
    "\n",
    "### Alternative distance measures\n",
    "\n",
    "1. Experiment with different distance measures, particularly Cosine distance. What happens to your clustering?\n",
    "\n",
    "\n",
    "### Reach Goals\n",
    "\n",
    "The standard k-means has some failings.  [kmeans++](http://en.wikipedia.org/wiki/K-means++) ([visualized](http://shabal.in/visuals/kmeans/KMeansPlusPlus.pdf)) solves the issue of picking the initial cluster centroids. PAM helps address this as well\n",
    "\n",
    "1. Fill out the PAM stub (KMedoids) and create plots parallelling that of KMeans\n",
    "2. Implement the kmeans++ initialization in your algorithm.\n",
    "3. Another variation on k-means is [bi-secting kmeans](http://stackoverflow.com/questions/6871489/bisecting-k-means-clustering-algorithm-explanation).  Adapt your code to implement bisecting kmeans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
